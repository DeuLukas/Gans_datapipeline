# etl_citydata
Creating an ETL data pipeline that supplies weather, airport and flight arrival data for select cities
As part of my datascience bootcamp a case study specifies the above requirements for subsequent data analysis for a fictional e-scooter company that aims to analyze and improve their scooter redeployment strategy. The project aims to pick up data engineering skills by setting up SQL schemas and filling it with a combination of webscraped and API data to deliver a clean and structured data warehouse that is cloud-hosted and auto-updated using google cloud. 
For google cloud automation local test code is provided in a Jupyter Notebook for both weather and flights data. Setting up google secrets and specifying all required packages in google cloud functions is required for full functionality. Surrounding steps in google cloud may require further tutorials for users that are inexperienced in google cloud. 
For local usage a .env file storing SQL root password and API keys paired with a config.py file enable operation without exposing sensitive information. Connection to SQL is enabled with associated 

## Webscraping with BeautifulSoup
The first task revolves around figuring out basic data about select cities. Using city names will prompt the algorithm to find out city-centre coordinates and other population data for the cities of interest. Functionality is achieved by analyzing html structure of wikipedia website for select cities. There are cities where functionality for population data and year of census are stored differently. In those cases -999 is inserted to indicate a missing value as the main goal of geolocation webscraping is achieved. 
